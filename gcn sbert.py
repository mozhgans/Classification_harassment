# -*- coding: utf-8 -*-
"""harassment classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12N3tS-t74Lk4PdrnULGiDuhnrUZmXinQ
"""

!pip install scikit-learn
!pip install sentence-transformers

pip install torch torchvision
pip install transformers
pip install sentence-transformers
pip install scikit-learn
pip install pandas
pip install numpy

#To implement tweet classification using a Graph Convolutional Network (GCN) with Sentence-BERT (SBERT) embeddings, we'll follow these steps:

#Load and preprocess the tweet dataset.
#Convert text tweets to SBERT embeddings.
#Build the graph representation of the tweets.
#Define and train the GCN model.
#Evaluate the model's performance for binary and multiclass classification.

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from transformers import BertTokenizer, BertModel
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Step 1: Load and preprocess the dataset
# Replace 'data.csv' with the path to your dataset file
data = pd.read_csv('data.csv')

# Step 2: Convert text tweets to SBERT embeddings
# Load SBERT model
sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')

# Convert tweets to SBERT embeddings
tweet_texts = data['tweet'].tolist()
tweet_embeddings = sbert_model.encode(tweet_texts, convert_to_tensor=True)

# Step 3: Build the graph representation of the tweets
# In this example, we will create a fully connected graph
num_tweets = len(data)
adj_matrix = torch.ones((num_tweets, num_tweets))

# Step 4: Define and train the GCN model
class GCN(nn.Module):
    def __init__(self, in_features, hidden_size, num_classes):
        super(GCN, self).__init__()
        self.conv1 = nn.Conv2d(1, hidden_size, (1, in_features))
        self.conv2 = nn.Conv2d(hidden_size, num_classes, (1, 1))
        self.activation = nn.ReLU()

    def forward(self, x, adj):
        x = x.unsqueeze(1)
        x = self.conv1(x)
        x = self.activation(x)
        x = torch.matmul(adj, x)
        x = self.conv2(x)
        x = self.activation(x)
        return x.squeeze(2)

# Replace in_features, hidden_size, and num_classes with appropriate values
in_features = tweet_embeddings.size(1)
hidden_size = 64
num_classes = 1  # For binary classification
# num_classes = 3  # For multiclass classification (indirect, physical, sexual harassment)

model = GCN(in_features, hidden_size, num_classes)
optimizer = optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.BCEWithLogitsLoss()  # For binary classification
# loss_fn = nn.CrossEntropyLoss()  # For multiclass classification

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    tweet_embeddings, data['label'], test_size=0.2, random_state=42
)

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    output = model(X_train, adj_matrix)
    # For binary classification, use sigmoid activation and BCEWithLogitsLoss
    output = torch.sigmoid(output)
    loss = loss_fn(output.squeeze(), y_train.to_numpy(dtype=float))
    loss.backward()
    optimizer.step()

# Step 5: Evaluate the model's performance
model.eval()
with torch.no_grad():
    test_output = model(X_test, adj_matrix)
    # For binary classification, use sigmoid activation and threshold
    test_output = torch.sigmoid(test_output)
    predictions = (test_output.squeeze() > 0.5).numpy()
    # For multiclass classification, use argmax to get predicted class
    # predictions = np.argmax(test_output.squeeze().numpy(), axis=1)

accuracy = accuracy_score(y_test, predictions)
print(f"Test Accuracy: {accuracy:.4f}")
print(classification_report(y_test, predictions))